{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_pickle_files(folder_path):\n",
    "    \"\"\"\n",
    "    Get all the .pickle files from a folder.\n",
    "\n",
    "    Args:\n",
    "    - folder_path (str): Path to the folder containing .pickle files\n",
    "\n",
    "    Returns:\n",
    "    - List of .pickle files\n",
    "    \"\"\"\n",
    "    pickle_files = [file for file in os.listdir(folder_path) if file.endswith('.pickle')]\n",
    "    return pickle_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_3d_tensors(radar_return, target_size=(100, 50)):\n",
    "    \"\"\"\n",
    "    Convert radar_return values to 3D torch tensors.\n",
    "\n",
    "    Args:\n",
    "    - radar_return (ndarray): 2D matrix consisting of complex numbers\n",
    "\n",
    "    Returns:\n",
    "    - 3D torch tensor with shape (width, height, channels)\n",
    "    \"\"\"\n",
    "    img = Image.fromarray(radar_return)\n",
    "    # Resize the image while preserving aspect ratio\n",
    "    img = img.resize(target_size, resample=Image.BILINEAR)\n",
    "    # Convert the PIL Image to NumPy array\n",
    "    resized_img = np.array(img)\n",
    "    # Split real and imaginary parts\n",
    "    real_part = resized_img.real\n",
    "    imag_part = resized_img.imag\n",
    "    # Stack real and imaginary parts to create a 3D array with two channels\n",
    "    array = np.stack((real_part, imag_part), axis=0)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def process_pickle_files(folder_path):\n",
    "    \"\"\"\n",
    "    Process all .pickle files in a folder.\n",
    "\n",
    "    Args:\n",
    "    - folder_path (str): Path to the folder containing .pickle files\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame containing all the samples\n",
    "    \"\"\"\n",
    "    # Get .pickle files\n",
    "    pickle_files = get_pickle_files(folder_path)\n",
    "    dataframes = []\n",
    "    for file_name in pickle_files:\n",
    "        with open(os.path.join(folder_path, file_name), 'rb') as f:\n",
    "            file_data = pickle.load(f)\n",
    "            radar_return = file_data['radar_return']\n",
    "            object_id = file_data['object_id']\n",
    "            concatenated_radar = convert_to_3d_tensors(radar_return).astype('float32')\n",
    "            # Create a DataFrame with concatenated radar and object_id\n",
    "            df = pd.DataFrame({'radar_return': [concatenated_radar], 'object_id': [object_id]})\n",
    "            # Append the DataFrame to the list\n",
    "            dataframes.append(df)\n",
    "\n",
    "    # Concatenate the list of DataFrames into a single DataFrame\n",
    "    df = pd.concat(dataframes, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "folder_path = 'Overfit_data'\n",
    "df = process_pickle_files(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Access a specific row (e.g., the first row)\n",
    "sample_row = df.iloc[2]\n",
    "\n",
    "# Access the 'radar_return' column of the sample row\n",
    "sample_image = sample_row['radar_return']\n",
    "\n",
    "# Display shape and values\n",
    "print(\"Shape of the sample image:\", sample_image.shape)\n",
    "print(\"Sample image values:\")\n",
    "print(sample_image)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_channels(tensor):\n",
    "    \"\"\"\n",
    "    Plot each channel of a 3D tensor as grayscale images.\n",
    "\n",
    "    Args:\n",
    "    - tensor (ndarray): 3D numpy array with shape (channels, height, width)\n",
    "    \"\"\"\n",
    "    num_channels = tensor.shape[0]\n",
    "    fig, axes = plt.subplots(1, num_channels, figsize=(5*num_channels, 5))\n",
    "    \n",
    "    for i in range(num_channels):\n",
    "        ax = axes[i] if num_channels > 1 else axes\n",
    "        ax.imshow(tensor[i], cmap='gray')\n",
    "        ax.set_title(f'Channel {i+1}')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "plot_channels(sample_image)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=2, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        # Define the max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Define the fully connected layers\n",
    "        self.fc1 = nn.Linear(32 * 16 * 16, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)  # Output size is 10 for classification (assuming 10 classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers with ReLU activation function and max pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Flatten the input for the fully connected layers\n",
    "        x = x.view(-1, 32 * 16 * 16)\n",
    "        # Apply fully connected layers with ReLU activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Apply the final fully connected layer without activation function\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "class RadarDataset(Dataset):\n",
    "    def __init__(self, df, target_size=(100, 50)):\n",
    "        self.df = df\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        radar_return = self.df.iloc[idx]['radar_return']\n",
    "        object_id = self.df.iloc[idx]['object_id']\n",
    "        # Convert radar return to 3D tensor\n",
    "        radar_return = self.convert_to_3d_tensors(radar_return)\n",
    "        return radar_return, object_id\n",
    "\n",
    "    def convert_to_3d_tensors(self, radar_return):\n",
    "        \"\"\"\n",
    "        Convert radar_return values to 3D tensor.\n",
    "\n",
    "        Args:\n",
    "        - radar_return (ndarray): 2D matrix consisting of complex numbers\n",
    "\n",
    "        Returns:\n",
    "        - 3D tensor with shape (channels, height, width)\n",
    "        \"\"\"\n",
    "        # Convert radar_return to PIL Image\n",
    "        img = Image.fromarray(radar_return)\n",
    "        # Resize the image while preserving aspect ratio using BILINEAR filter\n",
    "        img = img.resize(self.target_size, resample=Image.BILINEAR)\n",
    "        # Convert the resized image to NumPy array\n",
    "        resized_img = np.array(img)\n",
    "        # Split real and imaginary parts\n",
    "        real_part = resized_img.real\n",
    "        imag_part = resized_img.imag\n",
    "        # Stack real and imaginary parts to create a 3D array with two channels\n",
    "        array = np.stack((real_part, imag_part), axis=0)\n",
    "        # Convert the array to PyTorch tensor\n",
    "        tensor = torch.tensor(array, dtype=torch.float32)\n",
    "        return tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (conv1): Conv2d(2, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=8192, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleCNN().to(device)\n",
    "# Print the architecture\n",
    "print(model)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "dataset = RadarDataset(df)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 100), <f4",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\PycharmProjects\\model_implementation\\venv\\lib\\site-packages\\PIL\\Image.py:3098\u001B[0m, in \u001B[0;36mfromarray\u001B[1;34m(obj, mode)\u001B[0m\n\u001B[0;32m   3097\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3098\u001B[0m     mode, rawmode \u001B[38;5;241m=\u001B[39m \u001B[43m_fromarray_typemap\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtypekey\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m   3099\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[1;31mKeyError\u001B[0m: ((1, 1, 100), '<f4')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[39], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[0;32m      3\u001B[0m     running_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m----> 4\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloader, \u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m      5\u001B[0m         inputs, labels \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device), data[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      6\u001B[0m         optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[1;32m~\\PycharmProjects\\model_implementation\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\PycharmProjects\\model_implementation\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\PycharmProjects\\model_implementation\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\PycharmProjects\\model_implementation\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[1;32mIn[36], line 13\u001B[0m, in \u001B[0;36mRadarDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     11\u001B[0m object_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf\u001B[38;5;241m.\u001B[39miloc[idx][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mobject_id\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Convert radar return to 3D tensor\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m radar_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_to_3d_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mradar_return\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m radar_return, object_id\n",
      "Cell \u001B[1;32mIn[36], line 27\u001B[0m, in \u001B[0;36mRadarDataset.convert_to_3d_tensors\u001B[1;34m(self, radar_return)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;124;03mConvert radar_return values to 3D tensor.\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;124;03m- 3D tensor with shape (channels, height, width)\u001B[39;00m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m# Convert radar_return to PIL Image\u001B[39;00m\n\u001B[1;32m---> 27\u001B[0m img \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfromarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mradar_return\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m# Resize the image while preserving aspect ratio using BILINEAR filter\u001B[39;00m\n\u001B[0;32m     29\u001B[0m img \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39mresize(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_size, resample\u001B[38;5;241m=\u001B[39mImage\u001B[38;5;241m.\u001B[39mBILINEAR)\n",
      "File \u001B[1;32m~\\PycharmProjects\\model_implementation\\venv\\lib\\site-packages\\PIL\\Image.py:3102\u001B[0m, in \u001B[0;36mfromarray\u001B[1;34m(obj, mode)\u001B[0m\n\u001B[0;32m   3100\u001B[0m         typekey_shape, typestr \u001B[38;5;241m=\u001B[39m typekey\n\u001B[0;32m   3101\u001B[0m         msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot handle this data type: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtypekey_shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtypestr\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 3102\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m   3103\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   3104\u001B[0m     rawmode \u001B[38;5;241m=\u001B[39m mode\n",
      "\u001B[1;31mTypeError\u001B[0m: Cannot handle this data type: (1, 1, 100), <f4"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print every 100 mini-batches\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {running_loss/100}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
