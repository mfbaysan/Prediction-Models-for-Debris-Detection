GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name     | Type               | Params
------------------------------------------------
0 | lstm     | LSTM               | 316 K
1 | fc       | Linear             | 195
2 | accuracy | MulticlassAccuracy | 0
------------------------------------------------
316 K     Trainable params
0         Non-trainable params
316 K     Total params
1.267     Total estimated model params size (MB)
C:\Users\fatih\PycharmProjects\model_implementation\venv\lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
C:\Users\fatih\PycharmProjects\model_implementation\LSTMModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  val_acc = self.accuracy(F.softmax(logits), y)
C:\Users\fatih\PycharmProjects\model_implementation\venv\lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
C:\Users\fatih\PycharmProjects\model_implementation\venv\lib\site-packages\pytorch_lightning\loops\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
C:\Users\fatih\PycharmProjects\model_implementation\LSTMModel.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  train_acc = self.accuracy(F.softmax(logits), y)



Epoch 24: 100%|██████████| 1/1 [00:00<00:00,  6.21it/s, v_num=bgmm, train_loss_step=1.060, val_loss_step=1.260, val_loss_epoch=1.260, train_loss_epoch=1.060]



Epoch 29: 100%|██████████| 1/1 [00:00<00:00,  3.06it/s, v_num=bgmm, train_loss_step=1.060, val_loss_step=1.300, val_loss_epoch=1.300, train_loss_epoch=1.060]