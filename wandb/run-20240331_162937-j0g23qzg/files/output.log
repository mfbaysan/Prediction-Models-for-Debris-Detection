wandb: logging graph, to disable use `wandb.watch(log_graph=False)`
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
C:\Users\fatih\PycharmProjects\model_implementation\venv\lib\site-packages\pytorch_lightning\trainer\configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name     | Type               | Params
------------------------------------------------
0 | lstm     | LSTM               | 316 K
1 | fc       | Linear             | 130
2 | accuracy | MulticlassAccuracy | 0
------------------------------------------------
316 K     Trainable params
0         Non-trainable params
316 K     Total params
1.267     Total estimated model params size (MB)
C:\Users\fatih\PycharmProjects\model_implementation\venv\lib\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.
C:\Users\fatih\PycharmProjects\model_implementation\venv\lib\site-packages\pytorch_lightning\loops\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
C:\Users\fatih\PycharmProjects\model_implementation\LSTMModel.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  train_acc = self.accuracy(F.softmax(logits), y)








Epoch 99: 100%|██████████| 1/1 [00:00<00:00,  3.56it/s, v_num=3qzg, train_loss_step=0.693, train_loss_epoch=0.693]

Epoch 99: 100%|██████████| 1/1 [00:00<00:00,  2.73it/s, v_num=3qzg, train_loss_step=0.693, train_loss_epoch=0.693]